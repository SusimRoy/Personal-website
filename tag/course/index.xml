<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>course | Susim Mukul Roy</title><link>https://SusimRoy.github.io/tag/course/</link><atom:link href="https://SusimRoy.github.io/tag/course/index.xml" rel="self" type="application/rss+xml"/><description>course</description><generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 20 May 2023 00:00:00 +0000</lastBuildDate><image><url>https://SusimRoy.github.io/media/icon_hu0b7a4cb9992c9ac0e91bd28ffd38dd00_9727_512x512_fill_lanczos_center_3.png</url><title>course</title><link>https://SusimRoy.github.io/tag/course/</link></image><item><title>Deep Video Summarization</title><link>https://SusimRoy.github.io/project/project9/</link><pubDate>Sat, 20 May 2023 00:00:00 +0000</pubDate><guid>https://SusimRoy.github.io/project/project9/</guid><description>&lt;h2 id="summary">Summary&lt;/h2>
&lt;ul>
&lt;li>Given an input video data, we find the most informative slides and summarize the content in the video in the form of natural language.&lt;/li>
&lt;li>We first encode the images using the CLIP model and then pass it through a U-Net inspired transformer encoder-decoder architecture with skip connections in order to score each frame.&lt;/li>
&lt;li>Finally, the frame-level scores to shot-level scores and finally use dynamic programming (0/1 knapsack) to decide which shots to pick as keyshots.&lt;/li>
&lt;/ul></description></item><item><title>Deep Q Learning</title><link>https://SusimRoy.github.io/project/project8/</link><pubDate>Tue, 25 Apr 2023 00:00:00 +0000</pubDate><guid>https://SusimRoy.github.io/project/project8/</guid><description>&lt;h2 id="summary">Summary&lt;/h2>
&lt;ul>
&lt;li>The objective was to train a RL agent to play the world&amp;rsquo;s hardest game which is essentially to reach a goal point among arbitratrily moving obstacles.&lt;/li>
&lt;li>Created a simple MLP which projects from a n-dimensional state space to a m-dimensional action state which the RL agent should take.&lt;/li>
&lt;li>Created a custom reward function which penalizes the agent for being idle or getting stuck by an obstacle and rewards it for reaching the goal.&lt;/li>
&lt;/ul></description></item><item><title>Federated Learning</title><link>https://SusimRoy.github.io/project/project10/</link><pubDate>Thu, 20 Apr 2023 00:00:00 +0000</pubDate><guid>https://SusimRoy.github.io/project/project10/</guid><description>&lt;h2>Summary&lt;/h2>
- Implemented the FedAvg algorithm from scratch on three popular datasets, namely MNIST, Coloured-MNIST and SVHN. &lt;br/>
- Compared them with the case when they were trained and tested in a non-federated manner. &lt;br/></description></item><item><title>Feedback Approach to Foster Motion Information in FPAR</title><link>https://SusimRoy.github.io/project/project6/</link><pubDate>Sun, 02 Apr 2023 00:00:00 +0000</pubDate><guid>https://SusimRoy.github.io/project/project6/</guid><description>&lt;h2 id="summary">Summary&lt;/h2>
&lt;ul>
&lt;li>Improved the existing SparNet architecture by encorporating a feedback mechanism which basically embeds the finer information from the later layers of Resnet in it&amp;rsquo;s earlier layers.&lt;/li>
&lt;li>The Motion Prediction Block used the knowledge from the Action Recognition Block to incorporate it back into that in it&amp;rsquo;s first layer while taking care of the dimensions.&lt;/li>
&lt;li>Deployed the model on a webpage using flask framework where an user can input a video or an image and gets returned a heat map indicating the location of the action.&lt;/li>
&lt;/ul></description></item><item><title>Multimodal Art Database</title><link>https://SusimRoy.github.io/project/project11/</link><pubDate>Sun, 30 Oct 2022 00:00:00 +0000</pubDate><guid>https://SusimRoy.github.io/project/project11/</guid><description>&lt;h2>Summary&lt;/h2>
- A user had to input an artistic image and our software would detect the top k artists who could have possibly made the painting and store them in a local MariaDB database. &lt;br/>
- To find the similarity between input painting and the database, we used the CLIP model to first finetune on our art database and then using the cosine similarity between the stored embeddings and our input feature embedding. &lt;br/>
- The most likely images got stored in a normalized SQL tables with foreign keys linking between the image id table and the artist id table.</description></item><item><title>Pokemon Dashboard</title><link>https://SusimRoy.github.io/project/project7/</link><pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate><guid>https://SusimRoy.github.io/project/project7/</guid><description>&lt;h2 id="summary">Summary&lt;/h2>
&lt;ul>
&lt;li>A single webpage which shows different features of different pokemons and compares them with each other through multiple visualizations using interactive and informative graphics.&lt;/li>
&lt;li>Used interactive graphics like bubble plot along with radar plots and parallel graph plots to show distinctions between pokemons.&lt;/li>
&lt;/ul></description></item></channel></rss>